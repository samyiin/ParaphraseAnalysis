{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1852ade6-aa86-4193-9c8f-88ae0f76525d",
   "metadata": {},
   "source": [
    "We will start with the microsoft/deberta-v3-base model. And the MRPC subset of GLUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5ad287-0dc2-4274-943a-0ce6a0f17f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip3 install torch torchvision\n",
    "# ! pip install ipywidgets widgetsnbextension pandas-profiling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f2a470-85d7-477e-a1a6-e91be16a1a00",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "Here are the documentation:\n",
    "https://huggingface.co/transformers/v4.9.1/model_doc/deberta_v2.html\n",
    "For config:\n",
    "https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/configuration#transformers.PretrainedConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1fd6c77a-57f6-471f-85b9-fa4bbaff731e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "32b80d98-37cb-418c-ae6e-9e4ada77c483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.num_labels = 2\n",
    "\n",
    "# AutoModelForSequenceClassification is adding a classification head on top of the pretrained model. \n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474f2d2-c7a1-4122-8bcc-f8316e816eaf",
   "metadata": {},
   "source": [
    "*2024.06.17* I enountered a issue while loading the tokenizer, here is a post that solved the problem (make sure to restart the kernal): https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847  \n",
    "The \"use_fast\" parameter is from this post.  \n",
    "Another question is whether we should set the number of labels to be 1 or 2, since this is looking like a binary classification problem. But according to this website, 2 is also acceptable:  \n",
    "https://stackoverflow.com/questions/71768061/huggingface-transformers-classification-using-num-labels-1-vs-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26145898-c906-42f7-8b51-abc54d7d9a84",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa840529-7992-4427-b919-c842894169d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"nyu-mll/glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0bc03624-edab-4c9e-a3e4-2d5ea79d5385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First take a sample of the data\n",
    "train_dataset = datasets['train']\n",
    "sample_1 = train_dataset[0]\n",
    "sample_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafd129-0fd0-4a30-8b4a-dab6639174d9",
   "metadata": {},
   "source": [
    "### Representation\n",
    "The first problem I face is how to represent paraphrase: Paraphrase data is made of two sentences and a label indicationg whether they are paraphrase. So the label is clearly the output, what should the input look like? Here we found an good answer: https://huggingface.co/transformers/v3.0.2/glossary.html#token-type-ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "66579f69-d1f3-4cca-b61e-15dc3d7a91de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c322d59e-3e8d-426c-bcc6-9f5f5910038f",
   "metadata": {},
   "source": [
    "*2024.06.17* I encounter a problem: when I try to convert dataset to torch format, and then I try to access the first item of the train set, it shows this error:   \n",
    "ValueError: Unable to avoid copy while creating an array as requested.  \n",
    "So I found a solution in this website  \n",
    "https://support.gurobi.com/hc/en-us/articles/25787048531601-Compatibility-issues-with-numpy-2-0#:~:text=ValueError%3A%20Unable%20to%20avoid%20copy,4).  \n",
    "I fixed the problem by downgrading numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7579086f-8f9d-4ccd-ac68-494af041e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# tokenize the entire dataset: I make sure we pad every sentence (pair) to token length of 102\n",
    "# I first use tokenizer() to tokenize the entire train,test,val set separately, and see that the maximum length of tokens is 102. \n",
    "def tokenize(sample):\n",
    "    tokenized_dataset = tokenizer(\n",
    "        sample['sentence1'],\n",
    "        sample['sentence2'],  \n",
    "        truncation=True,               # Truncate sequences longer than the model's max length\n",
    "        padding='max_length',          # Pad to the maximum length\n",
    "        max_length = 102,              # I tried to pad them separately and see that the max length of token is 102\n",
    "        return_token_type_ids=True,    # Return token type IDs\n",
    "        return_attention_mask=True,    # Return attention mask\n",
    "    )\n",
    "    return tokenized_dataset\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize, batched =True) \n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['sentence1','sentence2','idx'])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb5063b-6075-4ec3-b36d-fdc5e0ae542a",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e71d7d34-e4ac-4f98-9740-0419dd4a17b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {set_name: DataLoader(dataset, batch_size=2) for set_name, dataset in tokenized_datasets.items()}\n",
    "# next(iter(dataloaders['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e5f42b63-6801-42a3-a8b3-981f40a448ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = None\n",
    "for batch in dataloaders['train']:\n",
    "    # the batch already includes the 'labels' (y) and the input_ids, masks, input_type_ids (x). \n",
    "    output = model(**batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "450e2bf2-6876-4ff6-a7ad-ba0d93351b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6932, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0660, -0.0719],\n",
       "        [-0.0627, -0.0683]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e06fa3-39c6-47d0-be39-2e8cdea2a031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
