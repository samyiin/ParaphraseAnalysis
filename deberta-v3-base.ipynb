{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1852ade6-aa86-4193-9c8f-88ae0f76525d",
   "metadata": {},
   "source": [
    "We will start with the microsoft/deberta-v3-base model. And the MRPC subset of GLUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5ad287-0dc2-4274-943a-0ce6a0f17f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.12/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./venv/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./venv/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip3 install torch torchvision\n",
    "# ! pip install ipywidgets widgetsnbextension pandas-profiling\n",
    "# ! pip install accelerate -U\n",
    "# !pip install evaluate\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f2a470-85d7-477e-a1a6-e91be16a1a00",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "Here are the documentation:\n",
    "https://huggingface.co/transformers/v4.9.1/model_doc/deberta_v2.html\n",
    "For config:\n",
    "https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/configuration#transformers.PretrainedConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b80d98-37cb-418c-ae6e-9e4ada77c483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samyiin/Projects/ParaphraseAnalysis/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# AutoModelForSequenceClassification is adding a classification head on top of the pretrained model. \n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.num_labels = 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474f2d2-c7a1-4122-8bcc-f8316e816eaf",
   "metadata": {},
   "source": [
    "*2024.06.17* I enountered a issue while loading the tokenizer, here is a post that solved the problem (make sure to restart the kernal): https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847  \n",
    "The \"use_fast\" parameter is from this post.  \n",
    "Another question is whether we should set the number of labels to be 1 or 2, since this is looking like a binary classification problem. But according to this website, 2 is also acceptable:  \n",
    "https://stackoverflow.com/questions/71768061/huggingface-transformers-classification-using-num-labels-1-vs-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26145898-c906-42f7-8b51-abc54d7d9a84",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa840529-7992-4427-b919-c842894169d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"nyu-mll/glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc03624-edab-4c9e-a3e4-2d5ea79d5385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First take a sample of the data\n",
    "train_dataset = datasets['train']\n",
    "sample_1 = train_dataset[0]\n",
    "sample_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafd129-0fd0-4a30-8b4a-dab6639174d9",
   "metadata": {},
   "source": [
    "### Representation\n",
    "The first problem I face is how to represent paraphrase: Paraphrase data is made of two sentences and a label indicationg whether they are paraphrase. So the label is clearly the output, what should the input look like? Here we found an good answer: https://huggingface.co/transformers/v3.0.2/glossary.html#token-type-ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66579f69-d1f3-4cca-b61e-15dc3d7a91de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c322d59e-3e8d-426c-bcc6-9f5f5910038f",
   "metadata": {},
   "source": [
    "*2024.06.17* I encounter a problem: when I try to convert dataset to torch format, and then I try to access the first item of the train set, it shows this error:   \n",
    "ValueError: Unable to avoid copy while creating an array as requested.  \n",
    "So I found a solution in this website  \n",
    "https://support.gurobi.com/hc/en-us/articles/25787048531601-Compatibility-issues-with-numpy-2-0#:~:text=ValueError%3A%20Unable%20to%20avoid%20copy,4).  \n",
    "I fixed the problem by downgrading numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7579086f-8f9d-4ccd-ac68-494af041e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# tokenize the entire dataset: I make sure we pad every sentence (pair) to token length of 102\n",
    "# I first use tokenizer() to tokenize the entire train,test,val set separately, and see that the maximum length of tokens is 102. \n",
    "def tokenize(sample):\n",
    "    tokenized_dataset = tokenizer(\n",
    "        sample['sentence1'],\n",
    "        sample['sentence2'],  \n",
    "        truncation=True,               # Truncate sequences longer than the model's max length\n",
    "        padding='max_length',          # Pad to the maximum length\n",
    "        max_length = 102,              # I tried to pad them separately and see that the max length of token is 102\n",
    "        return_token_type_ids=True,    # Return token type IDs\n",
    "        return_attention_mask=True,    # Return attention mask\n",
    "    )\n",
    "    return tokenized_dataset\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize, batched =True) \n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['sentence1','sentence2','idx'])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb5063b-6075-4ec3-b36d-fdc5e0ae542a",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db18b70b-d64a-4c26-acbf-fb7523819c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "metric = evaluate.load(\"accuracy\",)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return metric.compute(predictions=preds, references=p.label_ids)\n",
    "\n",
    "def record_config(sub_dir, hyper_parameters):\n",
    "    # record the hyper parameters\n",
    "    hyper_parameters_file = os.path.join(sub_dir, 'hyper_parameters.json')\n",
    "    with open(hyper_parameters_file, 'w') as f:\n",
    "        json.dump(hyper_parameters, f)\n",
    "\n",
    "def record_metrics(sub_dir, records, stage='train'):\n",
    "    json_file = os.path.join(sub_dir,f'{stage}_metrics.json')\n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(records, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5f42b63-6801-42a3-a8b3-981f40a448ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the EvalPrediction as in the trainer class's compute_metrics parameter. \n",
    "from transformers import EvalPrediction\n",
    "# to calculate the cross entropy\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_epochs(model, dataloaders, num_train_epochs, device, optimizer, scheduler):\n",
    "    model.to(device)\n",
    "    records = []\n",
    "    for epoch in range(num_train_epochs):\n",
    "        for step, batch in enumerate(dataloaders['train']):\n",
    "            # put everything on the right device\n",
    "            batch =  {k: v.to(device) for k, v in batch.items()}\n",
    "            # clear gradients, same old as usual\n",
    "            optimizer.zero_grad()\n",
    "            # the batch already includes the 'labels' (y) and the input_ids, masks, input_type_ids (x). \n",
    "            outputs = model(**batch)\n",
    "            # outputs.loss might be problematic because of the NllLossBackward0 without softmax, should use nn.CrossEntropy\n",
    "            loss = F.cross_entropy(input=outputs.logits, target=batch['labels'])\n",
    "            loss.backward()\n",
    "            # back propagation\n",
    "            optimizer.step()\n",
    "            # scheduler adjust lr\n",
    "            scheduler.step()\n",
    "            # record the loss and accuracy\n",
    "            pred_and_tags = EvalPrediction(predictions=outputs.logits.detach().numpy(), label_ids=batch['labels'])\n",
    "            record = {\"epoch\":epoch, \"batch\":step, \"loss\":loss.item(), \"accuracy\":compute_metrics(pred_and_tags)['accuracy']}\n",
    "            print(record)\n",
    "            records.append(record)\n",
    "    return model, records\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d3c1369-2329-489e-94cc-ef7ce7ef1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct training arguments, for now I am just changing  batch-size, number of epochs, learning rate, scheduler.\n",
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "import json\n",
    "\n",
    "def train_model(model, tokenized_datasets, hyper_parameters, tuned_parameters):\n",
    "    # get device\n",
    "    device = torch.device(hyper_parameters['device'])\n",
    "    \n",
    "    # batch size: the name is a bit weird because TrainingArguments can do it on multiple GPUs\n",
    "    per_device_train_batch_size = hyper_parameters['per_device_train_batch_size']\n",
    "    # initialize dataloaders\n",
    "    dataloaders = {}\n",
    "    dataloaders['train'] = DataLoader(tokenized_datasets['train'], batch_size=per_device_train_batch_size, shuffle=True)\n",
    "    # dataloaders['test'] = DataLoader(tokenized_datasets['test'], batch_size=per_device_train_batch_size)\n",
    "    # dataloaders['validation'] = DataLoader(tokenized_datasets['validation'], batch_size=per_device_train_batch_size)\n",
    "    \n",
    "    # initialize optimizer\n",
    "    learning_rate = hyper_parameters['learning_rate']\n",
    "    optimizer = AdamW(model.parameters(),lr=learning_rate)  # Here `model` is assumed to be instantiated\n",
    "    \n",
    "    # scheduler\n",
    "    lr_scheduler_type = hyper_parameters['lr_scheduler_type']\n",
    "    lr_scheduler_kwargs = { 'optimizer':optimizer, **hyper_parameters['lr_scheduler_kwargs']}\n",
    "    scheduler = get_scheduler(lr_scheduler_type, **lr_scheduler_kwargs)\n",
    "\n",
    "    # number of epochs\n",
    "    num_train_epochs = hyper_parameters['num_train_epochs']\n",
    "    \n",
    "    # Record the hyper parameters for this training\n",
    "    output_dir = hyper_parameters['output_dir']\n",
    "    # Create the base directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    # Create the subdirectory for the hyperparameters\n",
    "    sub_dir = os.path.join(output_dir, '_'.join([f\"{k}={hyper_parameters[k]}\" for k in tuned_parameters]))\n",
    "    if not os.path.exists(sub_dir):\n",
    "        os.makedirs(sub_dir)\n",
    "    record_config(sub_dir, hyper_parameters)\n",
    "\n",
    "    # train the model on the hyper parameters\n",
    "    model, records = train_epochs(model, dataloaders, num_train_epochs, device, optimizer, scheduler)\n",
    "    # save training metrics and model\n",
    "    record_metrics(sub_dir, records)\n",
    "    model.save_pretrained(sub_dir)\n",
    "    tokenizer.save_pretrained(sub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "626c0857-eaf3-4bd2-b86b-6a5986eaa44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'batch': 0, 'loss': 0.7226404547691345, 'accuracy': 0.375}\n",
      "{'epoch': 0, 'batch': 1, 'loss': 0.7395396828651428, 'accuracy': 0.28125}\n",
      "{'epoch': 1, 'batch': 0, 'loss': 0.7397270798683167, 'accuracy': 0.28125}\n",
      "{'epoch': 2, 'batch': 0, 'loss': 0.7262541651725769, 'accuracy': 0.34375}\n",
      "{'epoch': 3, 'batch': 0, 'loss': 0.7126044631004333, 'accuracy': 0.40625}\n",
      "{'epoch': 4, 'batch': 0, 'loss': 0.7263943552970886, 'accuracy': 0.3125}\n"
     ]
    }
   ],
   "source": [
    "# calculate the keyword args for schedular: I don't want to tune this, so set them by recomended.\n",
    "train_sample_size = len(tokenized_datasets['train'])\n",
    "num_train_epochs = 5\n",
    "per_device_train_batch_size = 32\n",
    "num_training_steps = int((train_sample_size/per_device_train_batch_size) * num_train_epochs)\n",
    "num_warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "\n",
    "# I will keep the name of hyper parameters consistent with TrainingArguments so that later I can switch to it. \n",
    "hyper_parameters = {\n",
    "    'learning_rate': 5e-05,\n",
    "    'per_device_train_batch_size': per_device_train_batch_size,\n",
    "    'lr_scheduler_type': 'linear',\n",
    "    'output_dir' : 'deberta_output/',\n",
    "    'num_train_epochs': num_train_epochs,\n",
    "    'lr_scheduler_kwargs': {'num_warmup_steps':num_warmup_steps,\n",
    "                            'num_training_steps':num_training_steps},\n",
    "    'device': \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "\n",
    "tuned_parameters = ['learning_rate','per_device_train_batch_size', 'lr_scheduler_type']\n",
    "\n",
    "\n",
    "train_model(model, tokenized_datasets, hyper_parameters, tuned_parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
